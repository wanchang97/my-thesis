\relax 
\abx@aux@refcontext{none/global//global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{acronym}{alg}{acr}{acn}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Reninforcement Learning}{2}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:Reinforcement Learning}{{1}{2}{Reninforcement Learning}{chapter.1}{}}
\newlabel{chap:Reinforcement Learning@cref}{{[chapter][1][]1}{[1][2][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Markov Decision Process}{2}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Basic Formulation and terminology}{2}{subsection.1.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Sequential Probabilistic Graphical Model of Markovian Decision Process}}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: MDPDiagramSequential}{{1.1}{2}{Sequential Probabilistic Graphical Model of Markovian Decision Process}{figure.caption.2}{}}
\newlabel{fig: MDPDiagramSequential@cref}{{[figure][1][1]1.1}{[1][2][]2}}
\@writefile{toc}{\contentsline {subsubsection}{State space $\mathds  {S}$ }{3}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Action space $\mathds  {A}$}{3}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Policy $\pi (a|s)$ }{3}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{State Transition Model $\mathbf  {T}$}{4}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Reward model $\mathbf  {R}$ (negative cost function)}{4}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The cumulative weighted total return $ U^{\pi }(s_t,a_t,\cdots  ,s_T,a_T) $}{4}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Expected cumulative weighted total return $J^{\pi }$}{4}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The State-Action Value function $Q^{\pi }(s_t,a_t)$}{5}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The State Value Function $V^{\pi }(s_t)$}{5}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Optimal State Action Value Function $Q^{\star }(s_t,a_t)$}{5}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Optimal State Value Function $V^{\star }(s_t)$}{6}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bellman Equation for State-Action Value Function $Q^{\pi }(s_t,a_t)$}{6}{section*.14}\protected@file@percent }
\newlabel{eqn: Bellman Qpi}{{1.8}{6}{Bellman Equation for State-Action Value Function $\Qpi (s_t,a_t)$}{equation.1.1.8}{}}
\newlabel{eqn: Bellman Qpi@cref}{{[subequation][8][1]1.8}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsubsection}{Bellman Equation for State Value Function $V^{\pi }(s_t)$}{7}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Optimal Bellman Equation for State Action Value Function $Q^{\star }(s_t,a_t)$}{8}{section*.16}\protected@file@percent }
\newlabel{eqn: MaxValueOfStateAction}{{1.19}{9}{Optimal Bellman Equation for State Action Value Function $\Qstar (s_t,a_t)$}{equation.1.1.19}{}}
\newlabel{eqn: MaxValueOfStateAction@cref}{{[subequation][19][1]1.19}{[1][8][]9}}
\@writefile{toc}{\contentsline {subsubsection}{Optimal Bellman Equation for State Value Function $V^{\star }(s_t)$}{9}{section*.17}\protected@file@percent }
\newlabel{eqn: MaxValueOfState}{{1.21}{9}{Optimal Bellman Equation for State Value Function $\Vstar (s_t)$}{equation.1.1.21}{}}
\newlabel{eqn: MaxValueOfState@cref}{{[subequation][21][1]1.21}{[1][9][]9}}
\newlabel{eqn:BellmanEqnOptimalValueFunction}{{1.22}{9}{Optimal Bellman Equation for State Value Function $\Vstar (s_t)$}{equation.1.1.22}{}}
\newlabel{eqn:BellmanEqnOptimalValueFunction@cref}{{[subequation][22][1]1.22}{[1][9][]9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Algorithm to solve a Markov Decision Process}{10}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Value Learning}{10}{subsection.1.1.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Example of a table estimation for the optimal state-action value function $Q(S,A)$}}{10}{table.caption.18}\protected@file@percent }
\newlabel{table: ExampleOfTableEstimationForQ}{{1.1}{10}{Example of a table estimation for the optimal state-action value function $Q(S,A)$}{table.caption.18}{}}
\newlabel{table: ExampleOfTableEstimationForQ@cref}{{[table][1][1]1.1}{[1][10][]10}}
\newlabel{eqn: GradientL}{{1.24}{11}{Value Learning}{equation.1.1.24}{}}
\newlabel{eqn: GradientL@cref}{{[equation][24][1]1.24}{[1][11][]11}}
\newlabel{eqn:ParameterUpdateQ}{{1.25}{11}{Value Learning}{equation.1.1.25}{}}
\newlabel{eqn:ParameterUpdateQ@cref}{{[equation][25][1]1.25}{[1][11][]11}}
\newlabel{eqn: MonteCarloTarget}{{1.26}{11}{Value Learning}{equation.1.1.26}{}}
\newlabel{eqn: MonteCarloTarget@cref}{{[equation][26][1]1.26}{[1][11][]11}}
\newlabel{eqn: mstepTDtargetQ}{{1.27}{11}{Value Learning}{equation.1.1.27}{}}
\newlabel{eqn: mstepTDtargetQ@cref}{{[equation][27][1]1.27}{[1][11][]11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Policy Learning}{11}{subsection.1.1.4}\protected@file@percent }
\newlabel{eqn:ParameterUpdateQpi}{{1.29}{11}{Policy Learning}{equation.1.1.29}{}}
\newlabel{eqn:ParameterUpdateQpi@cref}{{[equation][29][1]1.29}{[1][11][]11}}
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces Example of a table estimation for $Q^{\pi }(S,A)$}}{12}{table.caption.19}\protected@file@percent }
\newlabel{table: ExampleOfTableEstimationForQpi}{{1.2}{12}{Example of a table estimation for $Q^{\pi }(S,A)$}{table.caption.19}{}}
\newlabel{table: ExampleOfTableEstimationForQpi@cref}{{[table][2][1]1.2}{[1][11][]12}}
\newlabel{eqn: MonteCarloTargetQpi}{{1.30}{12}{Policy Learning}{equation.1.1.30}{}}
\newlabel{eqn: MonteCarloTargetQpi@cref}{{[equation][30][1]1.30}{[1][11][]12}}
\newlabel{eqn: mstepTDtargetQpi}{{1.31}{12}{Policy Learning}{equation.1.1.31}{}}
\newlabel{eqn: mstepTDtargetQpi@cref}{{[equation][31][1]1.31}{[1][12][]12}}
\newlabel{eqn:ParameterUpdateJ}{{1.34}{12}{Policy Learning}{equation.1.1.34}{}}
\newlabel{eqn:ParameterUpdateJ@cref}{{[equation][34][1]1.34}{[1][12][]12}}
\newlabel{eqn:GradientJ}{{1.35}{12}{Policy Learning}{equation.1.1.35}{}}
\newlabel{eqn:GradientJ@cref}{{[equation][35][1]1.35}{[1][12][]12}}
\abx@aux@cite{0}{info10040122}
\abx@aux@segm{0}{0}{info10040122}
\abx@aux@cite{0}{cybenko1989approximation}
\abx@aux@segm{0}{0}{cybenko1989approximation}
\abx@aux@cite{0}{hornik1989multilayer}
\abx@aux@segm{0}{0}{hornik1989multilayer}
\abx@aux@cite{0}{yarotsky2017error}
\abx@aux@segm{0}{0}{yarotsky2017error}
\abx@aux@cite{0}{cuomo2022scientific}
\abx@aux@segm{0}{0}{cuomo2022scientific}
\newlabel{eqn:GradientREINFORCEwithbaseline}{{1.38}{13}{Policy Learning}{equation.1.1.38}{}}
\newlabel{eqn:GradientREINFORCEwithbaseline@cref}{{[equation][38][1]1.38}{[1][13][]13}}
\newlabel{eqn:ACgradient}{{1.39}{13}{Policy Learning}{equation.1.1.39}{}}
\newlabel{eqn:ACgradient@cref}{{[equation][39][1]1.39}{[1][13][]13}}
\newlabel{eqn:GradientA2C}{{1.42}{13}{Policy Learning}{equation.1.1.42}{}}
\newlabel{eqn:GradientA2C@cref}{{[equation][42][1]1.42}{[1][13][]13}}
\newlabel{eqn:ParameterUpdateJcontinue}{{1.43}{13}{Policy Learning}{equation.1.1.43}{}}
\newlabel{eqn:ParameterUpdateJcontinue@cref}{{[equation][43][1]1.43}{[1][13][]13}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Deep Reinforcement Learning}{13}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Deep Q-Network (DQN)}{14}{subsection.1.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Deep Q-Network approximate the Q function for every available action using state as input $Q(s_t,a_t)$}}{14}{figure.caption.20}\protected@file@percent }
\newlabel{fig: NNapproximateQ}{{1.2}{14}{Deep Q-Network approximate the Q function for every available action using state as input $Q(s_t,a_t)$}{figure.caption.20}{}}
\newlabel{fig: NNapproximateQ@cref}{{[figure][2][1]1.2}{[1][14][]14}}
\abx@aux@cite{0}{mnih2013playing}
\abx@aux@segm{0}{0}{mnih2013playing}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Deep Q Network(DQN) Training}}{15}{algocf.1}\protected@file@percent }
\newlabel{alg: DQN}{{1}{15}{Deep Q-Network (DQN)}{algocf.1}{}}
\newlabel{alg: DQN@cref}{{[algorithm][1][]1}{[1][15][]15}}
\@writefile{toc}{\contentsline {subsubsection}{Disadvantages of DQN}{15}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Improve the training algorithm}{16}{section*.22}\protected@file@percent }
\newlabel{eqn: TDtargetofQ-learning}{{1.50}{16}{Improve the training algorithm}{equation.1.2.50}{}}
\newlabel{eqn: TDtargetofQ-learning@cref}{{[equation][50][1]1.50}{[1][16][]16}}
\newlabel{eqn: TDtargetofQ-learningWithTarget}{{1.51}{16}{Improve the training algorithm}{equation.1.2.51}{}}
\newlabel{eqn: TDtargetofQ-learningWithTarget@cref}{{[equation][51][1]1.51}{[1][16][]16}}
\newlabel{eqn: TDtargetofDoubleQlearning}{{1.52}{17}{Improve the training algorithm}{equation.1.2.52}{}}
\newlabel{eqn: TDtargetofDoubleQlearning@cref}{{[equation][52][1]1.52}{[1][16][]17}}
\@writefile{toc}{\contentsline {subsubsection}{Improved the Neural Network Architecture}{17}{section*.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Dueling Network architecture}}{18}{figure.caption.24}\protected@file@percent }
\newlabel{fig: Dueling Network approximate Q}{{1.3}{18}{Dueling Network architecture}{figure.caption.24}{}}
\newlabel{fig: Dueling Network approximate Q@cref}{{[figure][3][1]1.3}{[1][17][]18}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Deep Q Network(DQN) with Noisy Net Training}}{18}{algocf.2}\protected@file@percent }
\newlabel{alg: DQNwithNoisyNet}{{2}{18}{Improved the Neural Network Architecture}{algocf.2}{}}
\newlabel{alg: DQNwithNoisyNet@cref}{{[algorithm][2][]2}{[1][17][]18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Deep Policy Gradients}{19}{subsection.1.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces NN approximates $\pi (a_t|s_t)$}}{19}{figure.caption.25}\protected@file@percent }
\newlabel{fig: NNapproximate Pi}{{1.4}{19}{NN approximates $\pi (a_t|s_t)$}{figure.caption.25}{}}
\newlabel{fig: NNapproximate Pi@cref}{{[figure][4][1]1.4}{[1][19][]19}}
\@writefile{toc}{\contentsline {subsubsection}{Actor-Critic Algorithm}{19}{section*.27}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Actor Critic Neural Network architecture}}{20}{figure.caption.26}\protected@file@percent }
\newlabel{fig: NNapproximate AC}{{1.5}{20}{Actor Critic Neural Network architecture}{figure.caption.26}{}}
\newlabel{fig: NNapproximate AC@cref}{{[figure][5][1]1.5}{[1][19][]20}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Actor-Critic(AC)}}{20}{algocf.3}\protected@file@percent }
\newlabel{alg: AC}{{3}{20}{Actor-Critic Algorithm}{algocf.3}{}}
\newlabel{alg: AC@cref}{{[algorithm][3][]3}{[1][19][]20}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Actor-Critic(AC) with target network}}{21}{algocf.4}\protected@file@percent }
\newlabel{alg: ACwithTargetNetwork}{{4}{21}{Actor-Critic Algorithm}{algocf.4}{}}
\newlabel{alg: ACwithTargetNetwork@cref}{{[algorithm][4][]4}{[1][21][]21}}
\@writefile{toc}{\contentsline {subsubsection}{Advantage Actor-Critic (A2C) Algorithm}{22}{section*.28}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {5}{\ignorespaces Advantage Actor-Critic(A2C) with target network}}{22}{algocf.5}\protected@file@percent }
\newlabel{alg: A2C}{{5}{22}{Advantage Actor-Critic (A2C) Algorithm}{algocf.5}{}}
\newlabel{alg: A2C@cref}{{[algorithm][5][]5}{[1][21][]22}}
\@writefile{toc}{\contentsline {subsubsection}{Asynchronous Advantage Actor Critic (A3C) Algorithm}{22}{section*.29}\protected@file@percent }
\abx@aux@cite{0}{andriotis2019managing}
\abx@aux@segm{0}{0}{andriotis2019managing}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Multi-agent Actor Critic(MAC)}{23}{subsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Fully cooperative mode}{23}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Deep Centralized Multi-agent Actor Critic (DCMAC)}{23}{section*.31}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Deep Centralized Multi-agent Actor Critic Neural Network architecture}}{23}{figure.caption.32}\protected@file@percent }
\newlabel{fig: DCMAC}{{1.6}{23}{Deep Centralized Multi-agent Actor Critic Neural Network architecture}{figure.caption.32}{}}
\newlabel{fig: DCMAC@cref}{{[figure][6][1]1.6}{[1][23][]23}}
\abx@aux@cite{0}{andriotis2021deep}
\abx@aux@segm{0}{0}{andriotis2021deep}
\abx@aux@cite{0}{andriotis2023structural}
\abx@aux@segm{0}{0}{andriotis2023structural}
\@writefile{toc}{\contentsline {subsubsection}{Deep Decentralized Multi-agent Actor Critic (DDMAC)}{24}{section*.33}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Deep Decentralized Multi-agent Actor Critic Neural Network architecture}}{24}{figure.caption.34}\protected@file@percent }
\newlabel{fig: DDMAC}{{1.7}{24}{Deep Decentralized Multi-agent Actor Critic Neural Network architecture}{figure.caption.34}{}}
\newlabel{fig: DDMAC@cref}{{[figure][7][1]1.7}{[1][24][]24}}
\@writefile{toc}{\contentsline {subsubsection}{Hierarchical Resource Allocation and Continuous-control Reinforcement Learning}{24}{section*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Partially Observable Markov Decision Process}{25}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Basic Formulation and terminology}{25}{subsection.1.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Probabilistic Graphical Model of Partially Observable Markovian Decision Process defined in State Space}}{25}{figure.caption.36}\protected@file@percent }
\newlabel{fig: POMDPDiagramSequential}{{1.8}{25}{Probabilistic Graphical Model of Partially Observable Markovian Decision Process defined in State Space}{figure.caption.36}{}}
\newlabel{fig: POMDPDiagramSequential@cref}{{[figure][8][1]1.8}{[1][25][]25}}
\@writefile{toc}{\contentsline {subsubsection}{Belief Space $\mathbb  {B}$}{26}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Belief Update Rule}{27}{section*.38}\protected@file@percent }
\newlabel{eqn: BeliefUpdate}{{1.73}{27}{Belief Update Rule}{equation.1.3.73}{}}
\newlabel{eqn: BeliefUpdate@cref}{{[equation][73][1]1.73}{[1][27][]27}}
\newlabel{eqn: BeliefUpdateSimplified}{{1.74}{27}{Belief Update Rule}{equation.1.3.74}{}}
\newlabel{eqn: BeliefUpdateSimplified@cref}{{[equation][74][1]1.74}{[1][27][]27}}
\@writefile{toc}{\contentsline {subsubsection}{Belief-MDP}{27}{section*.39}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Probabilistic Graphical Model of Partially Observable Markovian Decision Process defined in Belief Space}}{28}{figure.caption.40}\protected@file@percent }
\newlabel{fig: BeliefMDPDiagramSequential}{{1.9}{28}{Probabilistic Graphical Model of Partially Observable Markovian Decision Process defined in Belief Space}{figure.caption.40}{}}
\newlabel{fig: BeliefMDPDiagramSequential@cref}{{[figure][9][1]1.9}{[1][27][]28}}
\@writefile{toc}{\contentsline {subsubsection}{Observation Model of Belief-MDP $P(o_{t+1}|\mathbf  {b}_t,a_t)$}{28}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{State Transition Model of Belief-MDP $\mathbf  {T}_{belief}$}{28}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Reward Model of Belief-MDP $\mathbf  {R}_{belief}$}{29}{section*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The cumulative weighted total return $ U^{\pi }(\mathbf  {b}_t,a_t,\cdots  ,\mathbf  {b}_T,a_T) $}{29}{section*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The State-Action Value Function $Q^{\pi }(\mathbf  {b}_t,a_t)$}{29}{section*.45}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The State Value Function $V^{\pi }(\mathbf  {b}_t)$}{30}{section*.46}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Optimal State Action Value Function $Q^{\star }(\mathbf  {b}_t,a_t)$}{30}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Optimal State Value Function $V^{\star }(\mathbf  {b}_t)$}{30}{section*.48}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Optimal State Action Value Function $V^{\star }(\mathbf  {b}_t)$}{30}{section*.49}\protected@file@percent }
\newlabel{eqn: MaxValueOfBeliefState}{{1.88}{31}{The Optimal State Action Value Function $\Vstar (\bb _t)$}{equation.1.3.88}{}}
\newlabel{eqn: MaxValueOfBeliefState@cref}{{[subequation][88][1]1.88}{[1][31][]31}}
\newlabel{eqn:BellmanEqnOptimalValueFunctionforBeliefState}{{1.89}{31}{The Optimal State Action Value Function $\Vstar (\bb _t)$}{equation.1.3.89}{}}
\newlabel{eqn:BellmanEqnOptimalValueFunctionforBeliefState@cref}{{[subequation][89][1]1.89}{[1][31][]31}}
\newlabel{eqn:OptimalValueFunctionAffineHyperplaneRepresentation}{{1.90}{31}{The Optimal State Action Value Function $\Vstar (\bb _t)$}{equation.1.3.90}{}}
\newlabel{eqn:OptimalValueFunctionAffineHyperplaneRepresentation@cref}{{[equation][90][1]1.90}{[1][31][]31}}
\abx@aux@cite{0}{johnson2018foundations}
\abx@aux@segm{0}{0}{johnson2018foundations}
\abx@aux@cite{0}{brown2021deeplearning}
\abx@aux@segm{0}{0}{brown2021deeplearning}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Algorithm to solve a Partially Observable Markov Decision Process}{32}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Common Implementations}{32}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Recent Advances}{32}{section.1.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.3}{\ignorespaces Comparison of traditional methods}}{33}{table.caption.50}\protected@file@percent }
\newlabel{tab:traditional_methods}{{1.3}{33}{Comparison of traditional methods}{table.caption.50}{}}
\newlabel{tab:traditional_methods@cref}{{[table][3][1]1.3}{[1][32][]33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Machine Learning Techniques}{33}{subsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Research Gaps}{33}{section.1.6}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{D41D8CD98F00B204E9800998ECF8427E}
\abx@aux@read@bblrerun
\gdef \@abspage@last{33}
